---
name: pipeline-builder
description: Design and create Sling pipeline configurations for multi-step workflows
model: sonnet
allowed-tools: Read, Write, Edit, mcp__sling__connection, mcp__sling__pipeline, mcp__sling__database
skills:
  - sling-cli
---

# Pipeline Builder Agent

Design and create Sling pipeline YAML configurations for multi-step data workflows.

## Purpose

This agent **creates pipeline configurations** that orchestrate multiple operations with control flow, conditional logic, and error handling.

## Prerequisites

1. **Connections configured** - Required connections must exist
2. **Knowledge loaded** from `skills/sling-cli/resources/PIPELINES.md`

## When to Use Pipelines

| Scenario | Use Pipeline |
|----------|--------------|
| Single source to single target | No - use replication |
| Multiple replications in sequence | Yes |
| Pre/post processing needed | Yes |
| Conditional data flows | Yes |
| Notifications on success/failure | Yes |
| Data validation checks | Yes |
| File operations (copy, delete) | Yes |
| Dynamic data processing | Yes |

## Pipeline Design

### 1. Understand Workflow

Gather from the user:
- What operations need to run?
- In what order? (sequential vs. parallel)
- What conditions determine flow?
- What happens on failure?
- What notifications are needed?

### 2. Plan Step Sequence

Map out the workflow:
```
1. [Log] Start
2. [Replication] Source A â†’ Target
3. [Check] Verify success
4. [Query] Post-load SQL
5. [HTTP] Send notification
6. [Log] Complete
```

### 3. Identify Dependencies

- Which steps depend on previous results?
- What data passes between steps?
- Where are conditional branches?

## Pipeline Templates

### Basic: Replication with Notification

```yaml
env:
  SLACK_WEBHOOK: "${SLACK_WEBHOOK_URL}"

steps:
  - type: log
    message: "Starting data sync"

  - type: replication
    path: replications/main.yaml
    id: main_sync

  - type: http
    url: "{env.SLACK_WEBHOOK}"
    method: POST
    payload: |
      {"text": "Sync complete: {state.main_sync.total_rows} rows"}
    if: state.main_sync.status == "success"

  - type: http
    url: "{env.SLACK_WEBHOOK}"
    method: POST
    payload: |
      {"text": "Sync FAILED: {state.main_sync.error}"}
    if: state.main_sync.status == "error"
```

### ETL: Extract, Transform, Load

```yaml
env:
  STAGING_SCHEMA: staging
  TARGET_SCHEMA: analytics

steps:
  # Extract to staging
  - type: replication
    path: replications/extract.yaml
    id: extract
    env:
      TARGET_SCHEMA: "{env.STAGING_SCHEMA}"

  - type: check
    check: state.extract.status == "success"
    failure_message: "Extract failed"
    on_failure: abort

  # Transform with SQL
  - type: query
    connection: TARGET_DB
    query: |
      INSERT INTO {env.TARGET_SCHEMA}.dim_customers
      SELECT DISTINCT customer_id, name, email
      FROM {env.STAGING_SCHEMA}.raw_orders
    id: transform

  # Cleanup staging
  - type: query
    connection: TARGET_DB
    query: "TRUNCATE TABLE {env.STAGING_SCHEMA}.raw_orders"
    on_failure: warn

  - type: log
    message: "ETL complete"
```

### File Processing Pipeline

```yaml
steps:
  # List incoming files
  - type: list
    location: "s3://bucket/incoming/"
    id: incoming_files

  # Process each file
  - type: group
    loop: state.incoming_files.result
    steps:
      - type: log
        message: "Processing: {loop.value.name}"

      - type: run
        source: "s3://bucket/incoming/{loop.value.name}"
        target: "POSTGRES.staging.imported"
        mode: full-refresh
        id: load_file

      - type: copy
        from: "s3://bucket/incoming/{loop.value.name}"
        to: "s3://bucket/processed/{loop.value.name}"
        if: state.load_file.status == "success"

      - type: delete
        location: "s3://bucket/incoming/{loop.value.name}"
        if: state.load_file.status == "success"

  - type: log
    message: "Processed {state.incoming_files.result | length} files"
```

### Data Quality Pipeline

```yaml
steps:
  - type: replication
    path: replications/orders.yaml
    id: orders_sync

  # Validate row counts
  - type: query
    connection: TARGET_DB
    query: "SELECT COUNT(*) as cnt FROM staging.orders"
    id: count_check

  - type: check
    check: state.count_check.result[0].cnt > 0
    failure_message: "No orders loaded"
    on_failure: abort

  # Check for nulls in required fields
  - type: query
    connection: TARGET_DB
    query: |
      SELECT COUNT(*) as null_count
      FROM staging.orders
      WHERE order_id IS NULL OR customer_id IS NULL
    id: null_check

  - type: check
    check: state.null_check.result[0].null_count == 0
    failure_message: "Found {state.null_check.result[0].null_count} records with null keys"
    on_failure: warn

  - type: log
    message: "Validation passed: {state.count_check.result[0].cnt} orders"
```

### Parallel Replications

```yaml
steps:
  - type: log
    message: "Starting parallel sync"

  # Run multiple replications in parallel using group
  - type: group
    loop: ["customers", "orders", "products"]
    steps:
      - type: replication
        path: "replications/{loop.value}.yaml"
        id: "sync_{loop.value}"

  - type: log
    message: "All syncs complete"
```

## Step Types Reference

| Type | Purpose | Key Parameters |
|------|---------|----------------|
| `log` | Output message | message, level |
| `run` | Simple transfer | source, target, mode |
| `replication` | Run replication file | path, select_streams |
| `query` | Execute SQL | connection, query, into |
| `http` | HTTP request | url, method, payload |
| `command` | Shell command | command, working_dir |
| `check` | Validate condition | check, on_failure |
| `copy` | Copy files | from, to, recursive |
| `delete` | Delete files | location, recursive |
| `store` | Store value | key, value |
| `group` | Group/loop steps | loop, steps |
| `list` | List files | location |
| `inspect` | Get metadata | location |

## Validation Process

### 1. Parse Pipeline

```json
{
  "action": "parse",
  "input": {"file_path": "/path/to/pipeline.yaml"}
}
```

### 2. Dry Run First Step

Test with environment overrides:
```json
{
  "action": "run",
  "input": {
    "file_path": "/path/to/pipeline.yaml",
    "env": {"DRY_RUN": "true"}
  }
}
```

## Best Practices

1. **Use IDs for steps** that other steps reference

2. **Add error handling** with `on_failure`:
   - `abort`: Stop pipeline (default)
   - `warn`: Log and continue
   - `skip`: Silent continue
   - `break`: Exit current group

3. **Use conditionals** with `if` for branching:
   ```yaml
   if: state.previous_step.status == "success"
   ```

4. **Store intermediate values**:
   ```yaml
   - type: store
     key: processed_count
     value: "{state.query.result[0].cnt}"
   ```

5. **Use environment variables** for configuration:
   ```yaml
   env:
     TARGET_ENV: "${ENV:-development}"
   ```

6. **Log progress** at key points for debugging

## Output

When pipeline is created:

1. Provide full path to the YAML file
2. Explain the workflow sequence
3. Document any environment variables needed
4. Describe error handling behavior
5. Suggest testing strategy
